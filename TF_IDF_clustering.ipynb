{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "VdqRAS3WPWo9"
      },
      "cell_type": "markdown",
      "source": [
        "# Example of TF-IDF clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic intuition"
      ],
      "metadata": {
        "id": "Fz4Y1LA_BQoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"purple is the best city in the forest\".split()\n",
        "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
        "c = \"it is not often you find soggy bananas on the street\".split()"
      ],
      "metadata": {
        "id": "Uk5gHKQIBUx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# we'll merge all docs into a list of lists for easier calculations below\n",
        "docs = [a, b, c]\n",
        "\n",
        "def tf_idf(word, sentence):\n",
        "    # term frequency\n",
        "    tf = sentence.count(word) / len(sentence)\n",
        "    print(f'TF={tf}')\n",
        "    # inverse document frequency\n",
        "    idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
        "    print(f'IDF={idf}')\n",
        "    print(f'TFIDF={round(tf*idf, 4)}')"
      ],
      "metadata": {
        "id": "Rc1jlnJTBak7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf('forest', a)\n"
      ],
      "metadata": {
        "id": "lpFGapu9BeHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf('forest', b)"
      ],
      "metadata": {
        "id": "oINrTcsdCiEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kN8mC7zS-4g"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.  Load and process text (for a simplified toy dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wikipedia\n",
        "# import wikipedia"
      ],
      "metadata": {
        "id": "vWqQtCYUFe04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wikipedia.set_lang(\"en\")\n",
        "# person = wikipedia.page(\"steven spielberg\")\n",
        "# print(person.content[:100])"
      ],
      "metadata": {
        "id": "ns9_nNMWGZt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# people = [\"michael jordan\", \"robert lewandowski\", \"dwight eisenhower\", \"woodrow wilson\", \"steven spielberg\"]\n",
        "# dat = [[person, wikipedia.page(person).content] for person in people]"
      ],
      "metadata": {
        "id": "FK0VJUvbHXB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NN0LWJXzTBfD"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "\n",
        "\n",
        "dat = [\n",
        "        ['X', 'aaa aaa'],\n",
        "        ['Y', 'aaa bbb bbb'],\n",
        "        ['Y', 'aaa bbb bbb ddd'],\n",
        "       ['Z', 'aaa bbb ccc eee'],\n",
        "       ['Z', 'aaa bbb ccc eee fff']\n",
        "     ]\n",
        "\n",
        "\n",
        "\n",
        "df_sentences = pd.DataFrame(dat, columns=['Label', 'Sentence'])\n",
        "\n",
        "labels = df_sentences['Label']\n",
        "print(\"labels:\")\n",
        "print(labels)\n",
        "\n",
        "n_clusters = np.unique(labels).shape[0]\n",
        "print(\"n_clusters:\", n_clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_clusters = 3"
      ],
      "metadata": {
        "id": "kxu6aLJzIA_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYqkfe2b9q2u"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Vectorize text to a numeric matrix      "
      ]
    },
    {
      "metadata": {
        "id": "XN5WeLgfTMKe"
      },
      "cell_type": "code",
      "source": [
        "sentences = df_sentences['Sentence'].values.tolist()\n",
        "vocab=None\n",
        "min_df=0.0\n",
        "max_df=1.0\n",
        "ngram_range=(1,1)\n",
        "\n",
        "\n",
        "# Build count vectorizer\n",
        "count_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, vocabulary=vocab, ngram_range=(1,1)) \n",
        "cvec = count_vectorizer.fit(sentences)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = cvec.get_feature_names_out()\n",
        "\n",
        "# Get bag-of-words and analyze\n",
        "bag_of_words = cvec.transform(sentences)\n",
        "df_bag_of_words = pd.DataFrame(bag_of_words.todense(), columns=feature_names)\n",
        "\n",
        "# Transform bag_of_words into tf-idf matrix\n",
        "transformer = TfidfTransformer()\n",
        "tfidf = transformer.fit_transform(bag_of_words)\n",
        "\n",
        "# Find most popular words and highest weights\n",
        "word_cnts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()  # for each word in column, sum all row counts\n",
        "df_cnts = pd.DataFrame({'word': feature_names, 'count': word_cnts})\n",
        "df_cnts = df_cnts.sort_values('count', ascending=False)\n",
        "\n",
        "# Build word weights as a list and sort them\n",
        "weights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\n",
        "df_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\n",
        "df_weights = df_weights.sort_values('weight', ascending=False)\n",
        "\n",
        "df_weights = df_weights.merge(df_cnts, on='word', how='left')\n",
        "df_weights = df_weights[['word', 'count', 'weight']]\n",
        "\n",
        "# Cosine similarity of sentences\n",
        "cos_sim = cosine_similarity(tfidf, tfidf)\n",
        "\n",
        "# Distance matrix of sentences\n",
        "samp_dist = 1 - cos_sim\n",
        "\n",
        "  \n",
        "# Build\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf.todense(), columns=feature_names)\n",
        "print(\"%d dummy sentences:\" % len(sentences))\n",
        "print(sentences)\n",
        "print(\"---\")\n",
        "\n",
        "\n",
        "print(\"%d feature_names (each feature represents a distinct word):\" % len(feature_names))\n",
        "print(feature_names)\n",
        "print(\"---\")\n",
        "print(\"df_tfidf[%d,%d]:\" % (len(sentences), len(feature_names)))\n",
        "print(df_tfidf.to_string())\n",
        "print(\"---\")\n",
        "print(\"df_weights:\")\n",
        "print(df_weights)\n",
        "print(\"---\")\n",
        "print(\"cos_sim[%d,%d] (a square matrix of length and width = len(sentences)):\" % (len(sentences), len(sentences)))\n",
        "print(cos_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sYn3f-mHg0z"
      },
      "cell_type": "markdown",
      "source": [
        "**Note how the above cosine similarity matrix corresponds to the original bag-of-words representation shown below:  **\n",
        "\n",
        "* Row 0 cosine similarity values are not similar to rows 1 to 4  \n",
        "* Rows 1 and 2 contain similar values (they are not identical due to the extra word in row 2: 'ddd')\n",
        "* Rows 3 and 4 contain identical values (with columns 3 and 4 shifted)"
      ]
    },
    {
      "metadata": {
        "id": "EoKia9q9QMeL"
      },
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words\n",
        "The bag-of-words representation will usually be sparser than this one (i.e. lots of zero values) since each sentence contains only a few of the words from the entire corpus"
      ]
    },
    {
      "metadata": {
        "id": "eDCU5tQtH1n6"
      },
      "cell_type": "code",
      "source": [
        "print(\"df_bag_of_words[%d,%d]:\" % (len(sentences), len(feature_names)))\n",
        "print(df_bag_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Y9n3H_wtBwE"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a word cloud from the weighted word counts"
      ]
    },
    {
      "metadata": {
        "id": "edqyqq53tBNz"
      },
      "cell_type": "code",
      "source": [
        "print(df_weights)\n",
        "s_word_freq = pd.Series(df_weights['count'])\n",
        "s_word_freq.index = df_weights['word']\n",
        "di_word_freq = s_word_freq.to_dict()\n",
        "\n",
        "print(\"---\")\n",
        "print(\"di_word_freq:\")\n",
        "for k,v in di_word_freq.items():\n",
        "  print(k,v)\n",
        "\n",
        "cloud = wordcloud.WordCloud(width=900, height=500).generate_from_frequencies(di_word_freq)\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHXmgodHyHBR"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Dimensionality Reduction using PCA"
      ]
    },
    {
      "metadata": {
        "id": "hs9P1aBhDJib"
      },
      "cell_type": "markdown",
      "source": [
        "Before attempting to cluster the data, we will usually want to reduce the dimensionality of the data because this helps to mitigate the problem of overfitting. Note the distinction between the two terms:\n",
        "\n",
        "* Dimensionality reduction: find the linear combinations of variables that are most 'interesting' in the data. For example, the polular PCA technique finds linear transformations of input features that maximize the variance of the data points along the new axes.\n",
        "\n",
        "* Clustering: find data points that can be grouped together as separate classes."
      ]
    },
    {
      "metadata": {
        "id": "jsbr2smZyGJq"
      },
      "cell_type": "code",
      "source": [
        "# Dimensionality reduction using PCA, reduce the tfidf matrix to just 2 features\n",
        "X = tfidf.todense()\n",
        "print(\"X before reduction:\")\n",
        "print(X)\n",
        "print('\\n\\n')\n",
        "\n",
        "X = np.array(X)\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "print(\"X_pca now has just 2 columns:\")\n",
        "print(X_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwt8PhGtfR7W"
      },
      "cell_type": "markdown",
      "source": [
        "##4. Calculate K-means clusters (unsupervised classification) "
      ]
    },
    {
      "metadata": {
        "id": "NXhl1YxUhwJJ"
      },
      "cell_type": "code",
      "source": [
        "km_model = KMeans(n_clusters=n_clusters, max_iter=10, n_init=2, random_state=121)\n",
        "\n",
        "# K-means (from number of features in input matrix to n_clusters)\n",
        "km_model.fit(X_pca)\n",
        "df_centers = pd.DataFrame(km_model.cluster_centers_, columns=['x', 'y'])\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.suptitle('PCA features colored by class; grey circles show the k-means centers')\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=km_model.labels_, s=50, cmap='jet')\n",
        "plt.scatter(df_centers['x'], df_centers['y'], c='grey', s=500, alpha=0.2);\n",
        "\n",
        "dy = 0.04\n",
        "for i, txt in enumerate(km_model.labels_):\n",
        "    my_label = df_sentences.iloc[i]['Label']\n",
        "    plt.annotate(my_label, (X_pca[i, 0], X_pca[i, 1] + dy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8nffo4bmaJd5"
      },
      "cell_type": "code",
      "source": [
        "print(\"km_model.labels_:\", km_model.labels_)\n",
        "print(\"This corresponds to the sentence labels shown below as follows:\")\n",
        "print(df_sentences['Label'].tolist())\n",
        "print(\"---\")\n",
        "print(\"df_centers:\")\n",
        "print(df_centers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo7TJWcsdSda"
      },
      "cell_type": "markdown",
      "source": [
        "**Note above how the center coordinates for the k-means model correspond to the original sentences shown below. The dimensions of the centers are: (n_clusters, n_features). Each row corresponds to one of the sentences. We often run PCA as the first step and therefore end up with 2 remaining features (n_features = 2). **"
      ]
    },
    {
      "metadata": {
        "id": "DAysWv3UdS04"
      },
      "cell_type": "code",
      "source": [
        "print(df_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KTAnibct435-"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ]
    },
    {
      "metadata": {
        "id": "vCRCYBm945dc"
      },
      "cell_type": "markdown",
      "source": [
        "The following data science techniques were demonstrated in the context of NLP (Natural Language Processing) using python's nltk library:\n",
        "\n",
        "* Vectorize text to a numeric matrix using TF-IDF\n",
        "\n",
        "* Dimensionality Reduction using PCA\n",
        "\n",
        "* Calculate K-means clusters (unsupervised classification) "
      ]
    }
  ]
}