{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfGytFjGeUJb"
      },
      "source": [
        "Importujemy biblioteki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4ul5RpcwSp"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6YLp3_Vehce"
      },
      "source": [
        "Tworzymy przykładowe dane:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB6GSXgdeXyV"
      },
      "source": [
        "docs = ['fantasticly done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "\t\t\n",
        "labels = np.array([[1.],[1.],[1.],[1.],[1.],[0.],[0.],[0.],[0.],[0.]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSSFxqWWexm9"
      },
      "source": [
        "Zamieniemy słowa na losowo wybrane liczby w zakresie do 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHq-50d6e4HU"
      },
      "source": [
        "vocab_size = 100\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTJpXpClfbQQ"
      },
      "source": [
        "Wyrównujemy wejście, żeby wszystkie były takie same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh7Ptn94fukD"
      },
      "source": [
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtP36m3VfyQ8"
      },
      "source": [
        "Definiujemy model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiSVEzzBe73I"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "#model.add(Dense(30, input_shape=(4,)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRczU455gA-9"
      },
      "source": [
        "Trenujemy i oceniamy model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5SE0GjKfoM4"
      },
      "source": [
        "model.fit(padded_docs, labels, epochs=3, verbose=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt2IZnjkhEoe"
      },
      "source": [
        "Prezentujemy embeddingsy na wykresie:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGsEa2TzhDhq"
      },
      "source": [
        "words = ['good','great','nice','excellent','weak','poor']\n",
        "encoded_word = [one_hot(d, vocab_size) for d in words]\n",
        "\n",
        "line = to_categorical(encoded_word, num_classes=vocab_size)\n",
        "\n",
        "\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "\n",
        "#show embeddings\n",
        "print(weights.shape)\n",
        "\n",
        "emb_words = np.dot(line, weights)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(emb_words)\n",
        "\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "In3L2d5bizc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}